2024-08-13 09:43:52.448 | INFO     | __main__:main:60 - ======================== Args ========================
2024-08-13 09:43:52.448 | INFO     | __main__:main:62 - structure               unet
2024-08-13 09:43:52.448 | INFO     | __main__:main:62 - downsampling_policy             lerp
2024-08-13 09:43:52.448 | INFO     | __main__:main:62 - upsampling_policy               lerp
2024-08-13 09:43:52.448 | INFO     | __main__:main:62 - num_iterations          16
2024-08-13 09:43:52.448 | INFO     | __main__:main:62 - relative_tolerance              1e-06
2024-08-13 09:43:52.448 | INFO     | __main__:main:62 - initialize_x0           random
2024-08-13 09:43:52.448 | INFO     | __main__:main:62 - num_mg_layers           6
2024-08-13 09:43:52.448 | INFO     | __main__:main:62 - num_mg_pre_smoothing            2
2024-08-13 09:43:52.448 | INFO     | __main__:main:62 - num_mg_post_smoothing           2
2024-08-13 09:43:52.448 | INFO     | __main__:main:62 - activation              none
2024-08-13 09:43:52.449 | INFO     | __main__:main:62 - initialize_trainable_parameters         default
2024-08-13 09:43:52.449 | INFO     | __main__:main:62 - optimizer               LBFGS
2024-08-13 09:43:52.449 | INFO     | __main__:main:62 - scheduler               ['step', '1', '0.99']
2024-08-13 09:43:52.449 | INFO     | __main__:main:62 - initial_lr              0.05
2024-08-13 09:43:52.449 | INFO     | __main__:main:62 - lambda_1                1
2024-08-13 09:43:52.449 | INFO     | __main__:main:62 - lambda_2                1
2024-08-13 09:43:52.449 | INFO     | __main__:main:62 - start_epoch             0
2024-08-13 09:43:52.449 | INFO     | __main__:main:62 - max_epoch               1
2024-08-13 09:43:52.449 | INFO     | __main__:main:62 - save_every              1
2024-08-13 09:43:52.449 | INFO     | __main__:main:62 - evaluate_every          5
2024-08-13 09:43:52.449 | INFO     | __main__:main:62 - dataset_root            /home/hadrien/Applications/mg_pytorch/evostencils/scripts/data/
2024-08-13 09:43:52.449 | INFO     | __main__:main:62 - num_workers             22
2024-08-13 09:43:52.449 | INFO     | __main__:main:62 - batch_size              1
2024-08-13 09:43:52.449 | INFO     | __main__:main:62 - device          cuda
2024-08-13 09:43:52.449 | INFO     | __main__:main:62 - checkpoint_root         /home/hadrien/Applications/mg_pytorch/evostencils/scripts/train/checkpoints
2024-08-13 09:43:52.449 | INFO     | __main__:main:62 - load_experiment         None
2024-08-13 09:43:52.450 | INFO     | __main__:main:62 - load_epoch              None
2024-08-13 09:43:52.450 | INFO     | __main__:main:62 - seed            1782213797
2024-08-13 09:43:52.450 | INFO     | __main__:main:62 - deterministic           False
2024-08-13 09:43:52.450 | INFO     | __main__:main:63 - ======================================================

2024-08-13 09:43:52.450 | INFO     | __main__:main:67 - [Train] Using device cuda
2024-08-13 09:43:52.450 | INFO     | __main__:main:76 - [Train] Do not enforce deterministic algorithms, cudnn benchmark enabled
2024-08-13 09:43:52.450 | INFO     | __main__:main:81 - [Train] Manual seed PyTorch with seed 1782213797

Parameter containing:
tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=torch.float64,
       requires_grad=True)
2024-08-13 09:43:52.777 | INFO     | evostencils.code_generation.trainer:__init__:138 - [Trainer] 10 training data loaded from /home/hadrien/Applications/mg_pytorch/evostencils/scripts/data/train
2024-08-13 09:43:52.777 | INFO     | evostencils.code_generation.trainer:__init__:148 - [Trainer] 0 evaluation data loaded from /home/hadrien/Applications/mg_pytorch/evostencils/scripts/data/evaluate

loss_x: tensor(7.5832, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0150, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0024, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0028, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0009, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0034, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0054, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0011, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0007, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0009, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
2024-08-13 09:44:18.912 | INFO     | evostencils.code_generation.trainer:train:209 - trainable omega = Parameter containing:
tensor([[0.7692, 0.7692, 0.7692, 0.7692, 0.7692, 0.7692, 0.7692, 0.7692, 0.7692,
         0.7692],
        [0.5862, 0.5862, 0.5862, 0.5862, 0.5862, 0.5862, 0.5862, 0.5862, 0.5862,
         0.5862],
        [1.1728, 1.1728, 1.1728, 1.1728, 1.1728, 1.1728, 1.1728, 1.1729, 1.1728,
         1.1732],
        [2.0452, 2.0452, 2.0452, 2.0452, 2.0452, 2.0452, 2.0452, 2.0452, 2.0452,
         2.0452],
        [2.0582, 2.0582, 2.0582, 2.0582, 2.0582, 2.0582, 2.0582, 2.0582, 2.0582,
         2.0582],
        [1.7549, 1.7549, 1.7549, 1.7549, 1.7549, 1.7549, 1.7549, 1.7549, 1.7549,
         1.7549],
        [1.5210, 1.5210, 1.5210, 1.5210, 1.5210, 1.5210, 1.5210, 1.5210, 1.5210,
         1.5210],
        [1.3577, 1.3577, 1.3577, 1.3577, 1.3577, 1.3577, 1.3577, 1.3577, 1.3577,
         1.3577],
        [1.0662, 1.0662, 1.0662, 1.0662, 1.0662, 1.0662, 1.0662, 1.0662, 1.0662,
         1.0662]], dtype=torch.float64, requires_grad=True), trainable weight = 0, time = 0.014311075210571289, conv_factor = 0.006417971517445066, iterations_used = 1
loss_x: tensor(0.0019, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0014, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0047, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0024, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0005, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0007, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0006, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0038, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0005, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0003, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
2024-08-13 09:44:44.787 | INFO     | evostencils.code_generation.trainer:train:209 - trainable omega = Parameter containing:
tensor([[0.7427, 0.7427, 0.7427, 0.7427, 0.7427, 0.7431, 0.7429, 0.7429, 0.7433,
         0.7414],
        [0.9772, 0.9772, 0.9772, 0.9772, 0.9772, 0.9772, 0.9772, 0.9772, 0.9772,
         0.9772],
        [1.3256, 1.3233, 0.9897, 1.3342, 1.2723, 1.1256, 1.1994, 1.3834, 1.3767,
         0.9518],
        [2.3858, 2.3858, 2.3858, 2.3858, 2.3858, 2.3858, 2.3858, 2.3858, 2.3858,
         2.3858],
        [1.6483, 1.6483, 1.6483, 1.6483, 1.6483, 1.6483, 1.6483, 1.6483, 1.6483,
         1.6483],
        [1.3447, 1.3447, 1.3447, 1.3447, 1.3447, 1.3447, 1.3447, 1.3447, 1.3447,
         1.3447],
        [1.6564, 1.6564, 1.6564, 1.6564, 1.6564, 1.6564, 1.6564, 1.6564, 1.6564,
         1.6564],
        [1.5366, 1.5366, 1.5366, 1.5366, 1.5366, 1.5366, 1.5366, 1.5366, 1.5366,
         1.5366],
        [1.1374, 1.1374, 1.1374, 1.1374, 1.1374, 1.1374, 1.1374, 1.1374, 1.1374,
         1.1374]], dtype=torch.float64, requires_grad=True), trainable weight = 0, time = 0.014252662658691406, conv_factor = 0.001036904287044206, iterations_used = 1
loss_x: tensor(9.4248e-05, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0006, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0014, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0092, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0033, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0124, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0030, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0021, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0006, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0036, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
2024-08-13 09:45:09.937 | INFO     | evostencils.code_generation.trainer:train:209 - trainable omega = Parameter containing:
tensor([[0.6051, 0.6052, 0.6052, 0.6052, 0.6053, 0.6045, 0.6049, 0.6049, 0.6041,
         0.6077],
        [0.9861, 0.9861, 0.9861, 0.9861, 0.9861, 0.9861, 0.9861, 0.9861, 0.9861,
         0.9861],
        [1.7271, 1.4725, 0.8833, 1.7534, 1.2456, 0.8237, 1.2257, 2.0945, 2.0002,
         0.7640],
        [2.4791, 2.4791, 2.4791, 2.4791, 2.4791, 2.4791, 2.4791, 2.4791, 2.4791,
         2.4791],
        [1.7137, 1.7137, 1.7137, 1.7137, 1.7137, 1.7137, 1.7137, 1.7137, 1.7137,
         1.7137],
        [0.8982, 0.8982, 0.8982, 0.8982, 0.8982, 0.8982, 0.8982, 0.8982, 0.8982,
         0.8982],
        [2.8766, 2.8766, 2.8766, 2.8766, 2.8766, 2.8766, 2.8766, 2.8766, 2.8766,
         2.8766],
        [1.3217, 1.3217, 1.3217, 1.3217, 1.3217, 1.3217, 1.3217, 1.3217, 1.3217,
         1.3217],
        [1.2011, 1.2011, 1.2011, 1.2011, 1.2011, 1.2011, 1.2011, 1.2011, 1.2011,
         1.2011]], dtype=torch.float64, requires_grad=True), trainable weight = 0, time = 0.013833045959472656, conv_factor = 0.0014989884925612103, iterations_used = 1
loss_x: tensor(0.0010, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0073, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0005, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0036, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0003, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0028, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0023, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0013, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0005, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0026, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
2024-08-13 09:45:35.595 | INFO     | evostencils.code_generation.trainer:train:209 - trainable omega = Parameter containing:
tensor([[0.7450, 0.7431, 0.7441, 0.7441, 0.7425, 0.7558, 0.7486, 0.7490, 0.7627,
         0.6992],
        [1.1654, 1.1654, 1.1654, 1.1654, 1.1654, 1.1654, 1.1654, 1.1654, 1.1654,
         1.1654],
        [1.2663, 1.2742, 0.8511, 1.6668, 1.6413, 0.7216, 1.2651, 3.0853, 3.0164,
         0.4429],
        [2.1733, 2.1733, 2.1733, 2.1733, 2.1733, 2.1733, 2.1733, 2.1733, 2.1733,
         2.1733],
        [2.2683, 2.2683, 2.2683, 2.2683, 2.2683, 2.2683, 2.2683, 2.2683, 2.2683,
         2.2683],
        [1.0474, 1.0474, 1.0474, 1.0474, 1.0474, 1.0474, 1.0474, 1.0474, 1.0474,
         1.0474],
        [2.6755, 2.6755, 2.6755, 2.6755, 2.6755, 2.6755, 2.6755, 2.6755, 2.6755,
         2.6755],
        [1.3837, 1.3837, 1.3837, 1.3837, 1.3837, 1.3837, 1.3837, 1.3837, 1.3837,
         1.3837],
        [1.0412, 1.0412, 1.0412, 1.0412, 1.0412, 1.0412, 1.0412, 1.0412, 1.0412,
         1.0412]], dtype=torch.float64, requires_grad=True), trainable weight = 0, time = 0.014580011367797852, conv_factor = 0.001256876406448379, iterations_used = 1
loss_x: tensor(0.0009, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0010, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0006, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0011, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0012, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0054, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0125, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0004, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0011, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0010, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
2024-08-13 09:46:01.492 | INFO     | evostencils.code_generation.trainer:train:209 - trainable omega = Parameter containing:
tensor([[0.6530, 0.6444, 0.6491, 0.6491, 0.6418, 0.7085, 0.6704, 0.6724, 0.7506,
         0.4995],
        [1.0392, 1.0392, 1.0392, 1.0392, 1.0392, 1.0392, 1.0392, 1.0392, 1.0393,
         1.0392],
        [1.0435, 1.4208, 0.8852, 2.3104, 3.4211, 0.7453, 1.4658, 6.3590, 6.0998,
         0.6778],
        [1.7796, 1.7796, 1.7796, 1.7796, 1.7796, 1.7796, 1.7796, 1.7796, 1.7796,
         1.7796],
        [1.7523, 1.7523, 1.7523, 1.7523, 1.7523, 1.7523, 1.7523, 1.7523, 1.7523,
         1.7523],
        [1.2307, 1.2307, 1.2307, 1.2307, 1.2307, 1.2307, 1.2307, 1.2307, 1.2307,
         1.2307],
        [1.5085, 1.5085, 1.5085, 1.5085, 1.5085, 1.5085, 1.5085, 1.5085, 1.5085,
         1.5085],
        [1.0524, 1.0524, 1.0524, 1.0524, 1.0524, 1.0524, 1.0524, 1.0524, 1.0524,
         1.0524],
        [0.9493, 0.9493, 0.9493, 0.9493, 0.9493, 0.9493, 0.9493, 0.9493, 0.9493,
         0.9493]], dtype=torch.float64, requires_grad=True), trainable weight = 0, time = 0.014046430587768555, conv_factor = 0.0007185449513197641, iterations_used = 1
loss_x: tensor(0.0124, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0007, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0006, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0002, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0006, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0010, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0003, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0007, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0003, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0070, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
2024-08-13 09:46:27.499 | INFO     | evostencils.code_generation.trainer:train:209 - trainable omega = Parameter containing:
tensor([[0.4870, 0.4741, 0.4813, 0.4814, 0.4700, 0.5534, 0.5107, 0.5132, 0.5907,
         0.5940],
        [0.8120, 0.8120, 0.8120, 0.8120, 0.8120, 0.8120, 0.8120, 0.8120, 0.8120,
         0.8120],
        [1.5838, 1.9999, 0.7848, 4.6502, 3.9972, 0.4901, 1.7750, 6.3572, 5.8919,
         0.7720],
        [0.9902, 0.9902, 0.9902, 0.9902, 0.9902, 0.9902, 0.9902, 0.9902, 0.9902,
         0.9902],
        [3.4114, 3.4114, 3.4114, 3.4114, 3.4114, 3.4114, 3.4114, 3.4114, 3.4114,
         3.4114],
        [1.6494, 1.6494, 1.6494, 1.6494, 1.6494, 1.6494, 1.6494, 1.6494, 1.6494,
         1.6494],
        [1.4399, 1.4399, 1.4399, 1.4399, 1.4399, 1.4399, 1.4399, 1.4399, 1.4399,
         1.4399],
        [1.2003, 1.2003, 1.2003, 1.2003, 1.2003, 1.2003, 1.2003, 1.2003, 1.2003,
         1.2003],
        [0.6113, 0.6113, 0.6113, 0.6113, 0.6113, 0.6113, 0.6113, 0.6113, 0.6113,
         0.6113]], dtype=torch.float64, requires_grad=True), trainable weight = 0, time = 0.014652252197265625, conv_factor = 0.002134053217975703, iterations_used = 1
loss_x: tensor(9.4506e-05, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0003, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(7.4175e-05, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0002, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0002, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0004, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0001, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0026, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0003, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0009, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
2024-08-13 09:46:53.155 | INFO     | evostencils.code_generation.trainer:train:209 - trainable omega = Parameter containing:
tensor([[0.5594, 0.4625, 0.5186, 0.5192, 0.4287, 1.0380, 0.4491, 0.6007, 1.4243,
         0.9021],
        [0.6936, 0.6936, 0.6936, 0.6936, 0.6936, 0.6936, 0.6936, 0.6936, 0.6936,
         0.6935],
        [1.5486, 1.4871, 0.8911, 5.9387, 5.9122, 0.5608, 1.4605, 6.1444, 5.9215,
         0.5031],
        [1.0492, 1.0492, 1.0492, 1.0492, 1.0492, 1.0492, 1.0492, 1.0492, 1.0492,
         1.0492],
        [3.0147, 3.0147, 3.0147, 3.0147, 3.0147, 3.0147, 3.0147, 3.0147, 3.0147,
         3.0147],
        [1.5634, 1.5634, 1.5634, 1.5634, 1.5634, 1.5634, 1.5634, 1.5634, 1.5634,
         1.5634],
        [1.0526, 1.0526, 1.0526, 1.0526, 1.0526, 1.0526, 1.0526, 1.0526, 1.0526,
         1.0526],
        [2.2892, 2.2892, 2.2892, 2.2892, 2.2892, 2.2892, 2.2892, 2.2892, 2.2892,
         2.2892],
        [1.0632, 1.0632, 1.0632, 1.0632, 1.0632, 1.0632, 1.0632, 1.0632, 1.0632,
         1.0632]], dtype=torch.float64, requires_grad=True), trainable weight = 0, time = 0.007490396499633789, conv_factor = 0.0019014375978505637, iterations_used = 1
loss_x: tensor(0.0003, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(7.4313e-05, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0003, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0004, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0001, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0005, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0001, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0011, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0024, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0004, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
2024-08-13 09:47:18.898 | INFO     | evostencils.code_generation.trainer:train:209 - trainable omega = Parameter containing:
tensor([[ 0.8039, -0.2736,  1.2473,  1.2311,  0.0584,  0.4418,  0.2675,  0.6498,
          1.4579,  0.5052],
        [ 0.9939,  0.9939,  0.9939,  0.9939,  0.9939,  0.9939,  0.9939,  0.9939,
          0.9939,  0.9939],
        [ 1.3178,  1.8950,  0.8107,  7.4520, 11.2050,  0.9157,  1.2557, 12.0144,
         11.7593,  0.6013],
        [ 1.1544,  1.1545,  1.1545,  1.1545,  1.1545,  1.1545,  1.1545,  1.1545,
          1.1545,  1.1545],
        [ 2.4398,  2.4398,  2.4398,  2.4398,  2.4398,  2.4398,  2.4398,  2.4398,
          2.4398,  2.4398],
        [ 1.7723,  1.7723,  1.7723,  1.7723,  1.7723,  1.7723,  1.7723,  1.7723,
          1.7723,  1.7723],
        [ 1.3406,  1.3406,  1.3406,  1.3406,  1.3406,  1.3406,  1.3406,  1.3406,
          1.3406,  1.3406],
        [ 1.4455,  1.4455,  1.4455,  1.4455,  1.4455,  1.4455,  1.4455,  1.4455,
          1.4455,  1.4455],
        [ 1.1437,  1.1437,  1.1437,  1.1437,  1.1437,  1.1437,  1.1437,  1.1437,
          1.1437,  1.1437]], dtype=torch.float64, requires_grad=True), trainable weight = 0, time = 0.01368856430053711, conv_factor = 0.00048052629889830715, iterations_used = 1
loss_x: tensor(0.0006, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0026, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0009, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0003, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0003, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(5.5894e-05, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0007, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0001, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0008, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0009, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
2024-08-13 09:47:44.838 | INFO     | evostencils.code_generation.trainer:train:209 - trainable omega = Parameter containing:
tensor([[ 0.6659, -0.2297,  1.0427,  1.0310,  0.5139,  0.1003,  0.7127,  0.4473,
          1.8863,  0.3451],
        [ 1.0170,  1.0170,  1.0170,  1.0170,  1.0170,  1.0170,  1.0170,  1.0170,
          1.0170,  1.0171],
        [ 1.2880,  2.6076,  0.9552,  5.4163, 12.7259,  0.7860,  1.2741, 13.4391,
         13.2662,  0.7188],
        [ 1.1706,  1.1707,  1.1707,  1.1707,  1.1707,  1.1707,  1.1707,  1.1707,
          1.1707,  1.1707],
        [ 2.1242,  2.1242,  2.1242,  2.1242,  2.1242,  2.1242,  2.1242,  2.1242,
          2.1242,  2.1242],
        [ 1.8555,  1.8555,  1.8555,  1.8555,  1.8555,  1.8555,  1.8555,  1.8555,
          1.8555,  1.8555],
        [ 1.2711,  1.2711,  1.2711,  1.2711,  1.2711,  1.2711,  1.2711,  1.2711,
          1.2711,  1.2711],
        [ 1.5367,  1.5367,  1.5367,  1.5367,  1.5367,  1.5367,  1.5367,  1.5367,
          1.5367,  1.5367],
        [ 1.1530,  1.1530,  1.1530,  1.1530,  1.1530,  1.1530,  1.1530,  1.1530,
          1.1530,  1.1530]], dtype=torch.float64, requires_grad=True), trainable weight = 0, time = 0.014563560485839844, conv_factor = 0.0005355627957949769, iterations_used = 1
loss_x: tensor(0.0029, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0004, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0005, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0013, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(8.4578e-05, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0004, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0002, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(5.0253e-05, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0002, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0005, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
2024-08-13 09:48:10.894 | INFO     | evostencils.code_generation.trainer:train:209 - trainable omega = Parameter containing:
tensor([[ 1.0527, -0.1792,  0.8674,  0.8325,  0.4772, -0.2044,  0.2476,  0.4266,
          0.8753,  0.4417],
        [ 1.0739,  1.0739,  1.0739,  1.0739,  1.0739,  1.0739,  1.0739,  1.0739,
          1.0739,  1.0742],
        [ 1.2492,  2.3018,  0.9118,  4.4294, 13.2858,  0.7943,  1.2865, 14.3724,
         14.4579,  0.5326],
        [ 1.1372,  1.1372,  1.1372,  1.1372,  1.1372,  1.1372,  1.1372,  1.1372,
          1.1372,  1.1372],
        [ 2.3931,  2.3931,  2.3931,  2.3931,  2.3931,  2.3931,  2.3931,  2.3931,
          2.3931,  2.3931],
        [ 1.6592,  1.6592,  1.6592,  1.6592,  1.6592,  1.6592,  1.6592,  1.6592,
          1.6592,  1.6592],
        [ 1.1049,  1.1049,  1.1049,  1.1049,  1.1049,  1.1049,  1.1049,  1.1049,
          1.1049,  1.1049],
        [ 1.9159,  1.9159,  1.9159,  1.9159,  1.9159,  1.9159,  1.9159,  1.9159,
          1.9159,  1.9159],
        [ 1.0031,  1.0031,  1.0031,  1.0031,  1.0031,  1.0031,  1.0031,  1.0031,
          1.0031,  1.0031]], dtype=torch.float64, requires_grad=True), trainable weight = 0, time = 0.01396322250366211, conv_factor = 0.002260112836440944, iterations_used = 1
2024-08-13 09:55:22.147 | INFO     | __main__:main:60 - ======================== Args ========================
2024-08-13 09:55:22.147 | INFO     | __main__:main:62 - structure               unet
2024-08-13 09:55:22.147 | INFO     | __main__:main:62 - downsampling_policy             lerp
2024-08-13 09:55:22.147 | INFO     | __main__:main:62 - upsampling_policy               lerp
2024-08-13 09:55:22.147 | INFO     | __main__:main:62 - num_iterations          16
2024-08-13 09:55:22.147 | INFO     | __main__:main:62 - relative_tolerance              1e-06
2024-08-13 09:55:22.147 | INFO     | __main__:main:62 - initialize_x0           random
2024-08-13 09:55:22.147 | INFO     | __main__:main:62 - num_mg_layers           6
2024-08-13 09:55:22.148 | INFO     | __main__:main:62 - num_mg_pre_smoothing            2
2024-08-13 09:55:22.148 | INFO     | __main__:main:62 - num_mg_post_smoothing           2
2024-08-13 09:55:22.148 | INFO     | __main__:main:62 - activation              none
2024-08-13 09:55:22.148 | INFO     | __main__:main:62 - initialize_trainable_parameters         default
2024-08-13 09:55:22.148 | INFO     | __main__:main:62 - optimizer               LBFGS
2024-08-13 09:55:22.148 | INFO     | __main__:main:62 - scheduler               ['step', '1', '0.99']
2024-08-13 09:55:22.148 | INFO     | __main__:main:62 - initial_lr              0.05
2024-08-13 09:55:22.148 | INFO     | __main__:main:62 - lambda_1                1
2024-08-13 09:55:22.148 | INFO     | __main__:main:62 - lambda_2                1
2024-08-13 09:55:22.148 | INFO     | __main__:main:62 - start_epoch             0
2024-08-13 09:55:22.148 | INFO     | __main__:main:62 - max_epoch               1
2024-08-13 09:55:22.148 | INFO     | __main__:main:62 - save_every              1
2024-08-13 09:55:22.148 | INFO     | __main__:main:62 - evaluate_every          5
2024-08-13 09:55:22.148 | INFO     | __main__:main:62 - dataset_root            /home/hadrien/Applications/mg_pytorch/evostencils/scripts/data/
2024-08-13 09:55:22.148 | INFO     | __main__:main:62 - num_workers             22
2024-08-13 09:55:22.148 | INFO     | __main__:main:62 - batch_size              1
2024-08-13 09:55:22.149 | INFO     | __main__:main:62 - device          cuda
2024-08-13 09:55:22.149 | INFO     | __main__:main:62 - checkpoint_root         /home/hadrien/Applications/mg_pytorch/evostencils/scripts/train/checkpoints
2024-08-13 09:55:22.149 | INFO     | __main__:main:62 - load_experiment         None
2024-08-13 09:55:22.149 | INFO     | __main__:main:62 - load_epoch              None
2024-08-13 09:55:22.149 | INFO     | __main__:main:62 - seed            513467613
2024-08-13 09:55:22.149 | INFO     | __main__:main:62 - deterministic           False
2024-08-13 09:55:22.149 | INFO     | __main__:main:63 - ======================================================

2024-08-13 09:55:22.149 | INFO     | __main__:main:67 - [Train] Using device cuda
2024-08-13 09:55:22.149 | INFO     | __main__:main:76 - [Train] Do not enforce deterministic algorithms, cudnn benchmark enabled
2024-08-13 09:55:22.149 | INFO     | __main__:main:81 - [Train] Manual seed PyTorch with seed 513467613

Parameter containing:
tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=torch.float64,
       requires_grad=True)
2024-08-13 09:55:22.493 | INFO     | evostencils.code_generation.trainer:__init__:138 - [Trainer] 10 training data loaded from /home/hadrien/Applications/mg_pytorch/evostencils/scripts/data/train
2024-08-13 09:55:22.493 | INFO     | evostencils.code_generation.trainer:__init__:148 - [Trainer] 0 evaluation data loaded from /home/hadrien/Applications/mg_pytorch/evostencils/scripts/data/evaluate

loss_x: tensor(20.3327, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0169, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0022, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0037, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0018, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(8.6911e-05, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0005, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0027, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0011, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0022, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
2024-08-13 09:55:48.516 | INFO     | evostencils.code_generation.trainer:train:209 - trainable omega = Parameter containing:
tensor([[0.8230, 0.8113, 0.8046, 0.7685, 0.7670, 0.7535, 0.8764, 1.0563, 0.5324,
         1.4895],
        [0.4534, 0.4534, 0.4534, 0.4534, 0.4534, 0.4534, 0.4534, 0.4534, 0.4534,
         0.4534],
        [1.1943, 1.1943, 1.1943, 1.1943, 1.1943, 1.1943, 1.1943, 1.1943, 1.1943,
         1.1943],
        [1.9844, 1.9844, 1.9844, 1.9844, 1.9844, 1.9844, 1.9844, 1.9844, 1.9844,
         1.9844],
        [1.8860, 1.8860, 1.8860, 1.8860, 1.8860, 1.8860, 1.8860, 1.8860, 1.8860,
         1.8860],
        [1.7522, 1.7522, 1.7522, 1.7522, 1.7522, 1.7522, 1.7522, 1.7522, 1.7522,
         1.7522],
        [1.4705, 1.4705, 1.4705, 1.4705, 1.4705, 1.4705, 1.4705, 1.4705, 1.4705,
         1.4705],
        [1.1921, 1.1921, 1.1921, 1.1921, 1.1921, 1.1921, 1.1921, 1.1921, 1.1921,
         1.1921],
        [1.0823, 1.0823, 1.0823, 1.0823, 1.0823, 1.0823, 1.0823, 1.0823, 1.0823,
         1.0823]], dtype=torch.float64, requires_grad=True), trainable weight = 0, time = 0.007372617721557617, conv_factor = 0.0015985229294216588, iterations_used = 1
loss_x: tensor(0.0008, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0006, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0009, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0020, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0014, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0007, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0110, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0104, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0045, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0172, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
2024-08-13 09:56:13.836 | INFO     | evostencils.code_generation.trainer:train:209 - trainable omega = Parameter containing:
tensor([[ 0.7831,  1.6329,  0.3018, -0.0480,  0.6188,  0.5040,  5.9682,  1.7284,
          1.1448,  0.5625],
        [ 1.0581,  1.0581,  1.0581,  1.0581,  1.0581,  1.0581,  1.0581,  1.0581,
          1.0581,  1.0581],
        [ 1.3218,  1.3218,  1.3218,  1.3218,  1.3218,  1.3218,  1.3218,  1.3218,
          1.3218,  1.3217],
        [ 2.2295,  2.2295,  2.2295,  2.2295,  2.2295,  2.2295,  2.2295,  2.2295,
          2.2295,  2.2295],
        [ 2.1205,  2.1205,  2.1205,  2.1205,  2.1205,  2.1205,  2.1205,  2.1205,
          2.1205,  2.1205],
        [ 1.3831,  1.3831,  1.3831,  1.3831,  1.3831,  1.3831,  1.3831,  1.3831,
          1.3831,  1.3831],
        [ 1.3103,  1.3103,  1.3103,  1.3103,  1.3103,  1.3103,  1.3103,  1.3103,
          1.3103,  1.3103],
        [ 2.1397,  2.1397,  2.1397,  2.1397,  2.1397,  2.1397,  2.1397,  2.1397,
          2.1397,  2.1397],
        [ 1.0638,  1.0638,  1.0638,  1.0638,  1.0638,  1.0638,  1.0638,  1.0638,
          1.0638,  1.0638]], dtype=torch.float64, requires_grad=True), trainable weight = 0, time = 0.013742923736572266, conv_factor = 0.003060050390311822, iterations_used = 1
loss_x: tensor(0.0055, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0043, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0039, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0051, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0020, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0011, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0009, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0008, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0049, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0026, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
2024-08-13 09:56:39.933 | INFO     | evostencils.code_generation.trainer:train:209 - trainable omega = Parameter containing:
tensor([[0.7483, 1.3459, 1.2234, 0.4861, 0.5356, 0.3805, 4.1569, 0.6763, 1.6434,
         0.9431],
        [1.0434, 1.0434, 1.0434, 1.0434, 1.0434, 1.0434, 1.0434, 1.0434, 1.0434,
         1.0434],
        [1.5338, 1.5338, 1.5339, 1.5338, 1.5338, 1.5339, 1.5339, 1.5339, 1.5339,
         1.5339],
        [1.6353, 1.6353, 1.6353, 1.6353, 1.6353, 1.6353, 1.6353, 1.6353, 1.6353,
         1.6353],
        [2.1822, 2.1822, 2.1822, 2.1822, 2.1822, 2.1822, 2.1822, 2.1822, 2.1822,
         2.1822],
        [1.4739, 1.4739, 1.4739, 1.4739, 1.4739, 1.4739, 1.4739, 1.4739, 1.4739,
         1.4739],
        [0.9904, 0.9904, 0.9904, 0.9904, 0.9904, 0.9904, 0.9904, 0.9904, 0.9904,
         0.9904],
        [1.7983, 1.7983, 1.7983, 1.7983, 1.7983, 1.7983, 1.7983, 1.7983, 1.7983,
         1.7983],
        [1.0040, 1.0040, 1.0040, 1.0040, 1.0040, 1.0040, 1.0040, 1.0040, 1.0040,
         1.0040]], dtype=torch.float64, requires_grad=True), trainable weight = 0, time = 0.014206171035766602, conv_factor = 0.0014783707254654346, iterations_used = 1
loss_x: tensor(0.0017, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0154, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0125, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0075, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0162, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0036, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0011, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0008, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0008, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0014, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
2024-08-13 09:57:05.442 | INFO     | evostencils.code_generation.trainer:train:209 - trainable omega = Parameter containing:
tensor([[ 0.3548,  1.8290, -2.8280,  1.0127,  0.5693,  0.5148,  8.2751,  0.5100,
          1.0688,  0.6249],
        [ 0.8750,  0.8750,  0.8750,  0.8750,  0.8750,  0.8750,  0.8750,  0.8750,
          0.8750,  0.8750],
        [ 0.8014,  0.8014,  0.8014,  0.8014,  0.8014,  0.8014,  0.8014,  0.8015,
          0.8015,  0.8015],
        [ 2.2604,  2.2604,  2.2604,  2.2604,  2.2604,  2.2604,  2.2604,  2.2604,
          2.2604,  2.2604],
        [ 1.7624,  1.7624,  1.7624,  1.7624,  1.7624,  1.7624,  1.7624,  1.7624,
          1.7624,  1.7624],
        [ 1.6987,  1.6987,  1.6987,  1.6987,  1.6987,  1.6987,  1.6987,  1.6987,
          1.6987,  1.6987],
        [ 1.5089,  1.5089,  1.5089,  1.5089,  1.5089,  1.5089,  1.5089,  1.5089,
          1.5089,  1.5089],
        [ 1.2827,  1.2827,  1.2827,  1.2827,  1.2827,  1.2827,  1.2827,  1.2827,
          1.2827,  1.2827],
        [ 0.9511,  0.9511,  0.9511,  0.9511,  0.9511,  0.9511,  0.9511,  0.9511,
          0.9511,  0.9511]], dtype=torch.float64, requires_grad=True), trainable weight = 0, time = 0.0070612430572509766, conv_factor = 0.0017704296429410786, iterations_used = 1
loss_x: tensor(0.0031, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0018, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0053, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0018, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0005, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0004, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0018, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0053, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0003, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0007, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
2024-08-13 09:57:30.882 | INFO     | evostencils.code_generation.trainer:train:209 - trainable omega = Parameter containing:
tensor([[ 0.4121,  2.0908, -2.0361,  1.0129,  0.8138,  0.5156,  5.6627,  0.4986,
          0.7479, -0.1857],
        [ 0.2041,  0.2041,  0.2041,  0.2041,  0.2041,  0.2041,  0.2041,  0.2041,
          0.2041,  0.2041],
        [ 1.0360,  1.0359,  1.0359,  1.0358,  1.0358,  1.0360,  1.0362,  1.0361,
          1.0361,  1.0374],
        [ 1.9628,  1.9628,  1.9628,  1.9628,  1.9628,  1.9628,  1.9628,  1.9628,
          1.9628,  1.9628],
        [ 2.3336,  2.3336,  2.3336,  2.3336,  2.3336,  2.3336,  2.3336,  2.3336,
          2.3336,  2.3336],
        [ 1.5220,  1.5220,  1.5220,  1.5220,  1.5220,  1.5220,  1.5220,  1.5220,
          1.5220,  1.5220],
        [ 0.9864,  0.9864,  0.9864,  0.9864,  0.9864,  0.9864,  0.9864,  0.9864,
          0.9864,  0.9864],
        [ 2.5877,  2.5877,  2.5877,  2.5877,  2.5877,  2.5877,  2.5877,  2.5877,
          2.5877,  2.5877],
        [ 1.0680,  1.0680,  1.0680,  1.0680,  1.0680,  1.0680,  1.0680,  1.0680,
          1.0680,  1.0680]], dtype=torch.float64, requires_grad=True), trainable weight = 0, time = 0.013630390167236328, conv_factor = 0.0073847156680314704, iterations_used = 1
loss_x: tensor(0.0025, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0008, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0017, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0003, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0006, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0002, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0001, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0009, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0003, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0003, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
2024-08-13 09:57:55.767 | INFO     | evostencils.code_generation.trainer:train:209 - trainable omega = Parameter containing:
tensor([[ 0.5668,  1.8917, -0.0485,  0.9809,  0.4167,  0.7987, 12.9360,  0.5211,
          1.0446,  0.5109],
        [ 0.1795,  0.1795,  0.1795,  0.1795,  0.1795,  0.1795,  0.1795,  0.1795,
          0.1795,  0.1795],
        [ 0.5647,  0.5647,  0.5647,  0.5647,  0.5646,  0.5647,  0.5648,  0.5648,
          0.5648,  0.5654],
        [ 1.5209,  1.5209,  1.5209,  1.5209,  1.5209,  1.5209,  1.5209,  1.5209,
          1.5209,  1.5209],
        [ 2.4450,  2.4450,  2.4450,  2.4450,  2.4450,  2.4450,  2.4450,  2.4450,
          2.4450,  2.4450],
        [ 1.7465,  1.7465,  1.7465,  1.7465,  1.7465,  1.7465,  1.7465,  1.7465,
          1.7465,  1.7465],
        [ 1.4924,  1.4924,  1.4924,  1.4924,  1.4924,  1.4924,  1.4924,  1.4924,
          1.4924,  1.4924],
        [ 1.3405,  1.3405,  1.3405,  1.3405,  1.3405,  1.3405,  1.3405,  1.3405,
          1.3405,  1.3405],
        [ 1.0580,  1.0580,  1.0580,  1.0580,  1.0580,  1.0580,  1.0580,  1.0580,
          1.0580,  1.0580]], dtype=torch.float64, requires_grad=True), trainable weight = 0, time = 0.014234781265258789, conv_factor = 0.006035896168114217, iterations_used = 1
loss_x: tensor(0.0003, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0041, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0003, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0004, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0005, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0014, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0005, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0008, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0002, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(6.8279e-05, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
2024-08-13 09:58:21.351 | INFO     | evostencils.code_generation.trainer:train:209 - trainable omega = Parameter containing:
tensor([[0.6613, 1.7160, 0.4735, 0.5205, 0.0235, 1.0189, 3.8133, 0.4274, 1.5388,
         0.5881],
        [0.0618, 0.0618, 0.0618, 0.0618, 0.0618, 0.0618, 0.0618, 0.0618, 0.0618,
         0.0618],
        [1.0195, 1.0166, 1.0175, 1.0136, 1.0103, 1.0189, 1.0306, 1.0273, 1.0239,
         1.0814],
        [2.1663, 2.1663, 2.1663, 2.1663, 2.1663, 2.1663, 2.1663, 2.1663, 2.1663,
         2.1663],
        [1.7542, 1.7542, 1.7542, 1.7542, 1.7542, 1.7542, 1.7542, 1.7542, 1.7542,
         1.7542],
        [1.6344, 1.6344, 1.6344, 1.6344, 1.6344, 1.6344, 1.6344, 1.6344, 1.6344,
         1.6344],
        [1.0465, 1.0465, 1.0465, 1.0465, 1.0465, 1.0465, 1.0465, 1.0465, 1.0465,
         1.0465],
        [2.3561, 2.3561, 2.3561, 2.3561, 2.3561, 2.3561, 2.3561, 2.3561, 2.3561,
         2.3561],
        [1.0809, 1.0809, 1.0809, 1.0809, 1.0809, 1.0809, 1.0809, 1.0809, 1.0809,
         1.0809]], dtype=torch.float64, requires_grad=True), trainable weight = 0, time = 0.013495683670043945, conv_factor = 0.0038656812010685695, iterations_used = 1
loss_x: tensor(0.0008, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0004, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0007, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0009, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0005, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0006, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0008, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0007, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0010, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0031, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
2024-08-13 09:58:46.054 | INFO     | evostencils.code_generation.trainer:train:209 - trainable omega = Parameter containing:
tensor([[ 0.9315,  2.0635,  0.5010,  0.6299,  1.2046,  1.1677,  4.1755,  0.5465,
         -0.3199,  0.6584],
        [-0.0076, -0.0076, -0.0076, -0.0076, -0.0076, -0.0076, -0.0076, -0.0076,
         -0.0076, -0.0076],
        [ 1.0115,  1.0152,  1.0140,  1.0192,  1.0236,  1.0121,  0.9981,  1.0018,
          1.0060,  1.0122],
        [ 2.1641,  2.1641,  2.1641,  2.1641,  2.1641,  2.1641,  2.1641,  2.1641,
          2.1641,  2.1641],
        [ 1.6962,  1.6962,  1.6962,  1.6962,  1.6962,  1.6962,  1.6962,  1.6962,
          1.6962,  1.6962],
        [ 1.7661,  1.7661,  1.7661,  1.7661,  1.7661,  1.7661,  1.7661,  1.7661,
          1.7661,  1.7661],
        [ 1.2902,  1.2902,  1.2902,  1.2902,  1.2902,  1.2902,  1.2902,  1.2902,
          1.2902,  1.2902],
        [ 1.6701,  1.6701,  1.6701,  1.6701,  1.6701,  1.6701,  1.6701,  1.6701,
          1.6701,  1.6701],
        [ 1.0637,  1.0637,  1.0637,  1.0637,  1.0637,  1.0637,  1.0637,  1.0637,
          1.0637,  1.0637]], dtype=torch.float64, requires_grad=True), trainable weight = 0, time = 0.013851165771484375, conv_factor = 0.0026988893075093057, iterations_used = 1
loss_x: tensor(0.0004, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0005, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0016, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0019, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0011, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0019, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0003, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0016, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0014, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0013, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
2024-08-13 09:59:11.408 | INFO     | evostencils.code_generation.trainer:train:209 - trainable omega = Parameter containing:
tensor([[ 1.1823,  2.0228,  0.5686,  0.7026,  1.3026,  1.0872,  4.6135,  0.4903,
          0.4779,  0.7263],
        [-0.0183, -0.0183, -0.0183, -0.0183, -0.0183, -0.0183, -0.0183, -0.0183,
         -0.0183, -0.0183],
        [ 1.0484,  1.0582,  1.0554,  1.0681,  1.0782,  1.0507,  1.0084,  1.0204,
          1.0327,  0.8012],
        [ 2.2831,  2.2831,  2.2831,  2.2831,  2.2831,  2.2831,  2.2831,  2.2831,
          2.2831,  2.2831],
        [ 1.9113,  1.9113,  1.9113,  1.9113,  1.9113,  1.9113,  1.9113,  1.9113,
          1.9113,  1.9113],
        [ 1.5995,  1.5995,  1.5995,  1.5995,  1.5995,  1.5995,  1.5995,  1.5995,
          1.5995,  1.5995],
        [ 1.2909,  1.2909,  1.2909,  1.2909,  1.2909,  1.2909,  1.2909,  1.2909,
          1.2909,  1.2909],
        [ 1.7052,  1.7052,  1.7052,  1.7052,  1.7052,  1.7052,  1.7052,  1.7052,
          1.7052,  1.7052],
        [ 1.0171,  1.0171,  1.0171,  1.0171,  1.0171,  1.0171,  1.0171,  1.0171,
          1.0171,  1.0171]], dtype=torch.float64, requires_grad=True), trainable weight = 0, time = 0.014238595962524414, conv_factor = 0.0006162283940420321, iterations_used = 1
loss_x: tensor(7.3176e-05, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0010, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0008, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0033, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0010, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0007, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0015, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0009, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0054, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0004, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
2024-08-13 09:59:36.784 | INFO     | evostencils.code_generation.trainer:train:209 - trainable omega = Parameter containing:
tensor([[1.0873, 0.5831, 0.2386, 0.5436, 1.2982, 2.2826, 6.5751, 0.4903, 0.4909,
         0.9209],
        [0.0595, 0.0595, 0.0595, 0.0595, 0.0595, 0.0595, 0.0595, 0.0595, 0.0595,
         0.0595],
        [1.0849, 1.0691, 1.0704, 1.0478, 1.0198, 1.0762, 1.1008, 1.1108, 1.1076,
         0.8924],
        [1.9868, 1.9868, 1.9868, 1.9868, 1.9868, 1.9868, 1.9868, 1.9868, 1.9868,
         1.9868],
        [2.0247, 2.0247, 2.0247, 2.0247, 2.0247, 2.0247, 2.0247, 2.0247, 2.0247,
         2.0247],
        [1.6942, 1.6942, 1.6942, 1.6942, 1.6942, 1.6942, 1.6942, 1.6942, 1.6942,
         1.6942],
        [1.3903, 1.3903, 1.3903, 1.3903, 1.3903, 1.3903, 1.3903, 1.3903, 1.3903,
         1.3903],
        [1.4953, 1.4953, 1.4953, 1.4953, 1.4953, 1.4953, 1.4953, 1.4953, 1.4953,
         1.4953],
        [1.0669, 1.0669, 1.0669, 1.0669, 1.0669, 1.0669, 1.0669, 1.0669, 1.0669,
         1.0669]], dtype=torch.float64, requires_grad=True), trainable weight = 0, time = 0.013783454895019531, conv_factor = 0.0011425718259214233, iterations_used = 1
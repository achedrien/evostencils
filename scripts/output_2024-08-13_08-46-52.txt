2024-08-13 09:41:12.950 | INFO     | __main__:main:60 - ======================== Args ========================
2024-08-13 09:41:12.950 | INFO     | __main__:main:62 - structure               unet
2024-08-13 09:41:12.950 | INFO     | __main__:main:62 - downsampling_policy             lerp
2024-08-13 09:41:12.950 | INFO     | __main__:main:62 - upsampling_policy               lerp
2024-08-13 09:41:12.950 | INFO     | __main__:main:62 - num_iterations          16
2024-08-13 09:41:12.950 | INFO     | __main__:main:62 - relative_tolerance              1e-06
2024-08-13 09:41:12.950 | INFO     | __main__:main:62 - initialize_x0           random
2024-08-13 09:41:12.950 | INFO     | __main__:main:62 - num_mg_layers           6
2024-08-13 09:41:12.950 | INFO     | __main__:main:62 - num_mg_pre_smoothing            2
2024-08-13 09:41:12.950 | INFO     | __main__:main:62 - num_mg_post_smoothing           2
2024-08-13 09:41:12.950 | INFO     | __main__:main:62 - activation              none
2024-08-13 09:41:12.950 | INFO     | __main__:main:62 - initialize_trainable_parameters         default
2024-08-13 09:41:12.950 | INFO     | __main__:main:62 - optimizer               LBFGS
2024-08-13 09:41:12.950 | INFO     | __main__:main:62 - scheduler               ['step', '1', '0.99']
2024-08-13 09:41:12.951 | INFO     | __main__:main:62 - initial_lr              0.05
2024-08-13 09:41:12.951 | INFO     | __main__:main:62 - lambda_1                1
2024-08-13 09:41:12.951 | INFO     | __main__:main:62 - lambda_2                1
2024-08-13 09:41:12.951 | INFO     | __main__:main:62 - start_epoch             0
2024-08-13 09:41:12.951 | INFO     | __main__:main:62 - max_epoch               1
2024-08-13 09:41:12.951 | INFO     | __main__:main:62 - save_every              1
2024-08-13 09:41:12.951 | INFO     | __main__:main:62 - evaluate_every          5
2024-08-13 09:41:12.951 | INFO     | __main__:main:62 - dataset_root            /home/hadrien/Applications/mg_pytorch/evostencils/scripts/data/
2024-08-13 09:41:12.951 | INFO     | __main__:main:62 - num_workers             22
2024-08-13 09:41:12.951 | INFO     | __main__:main:62 - batch_size              1
2024-08-13 09:41:12.951 | INFO     | __main__:main:62 - device          cuda
2024-08-13 09:41:12.951 | INFO     | __main__:main:62 - checkpoint_root         /home/hadrien/Applications/mg_pytorch/evostencils/scripts/train/checkpoints
2024-08-13 09:41:12.951 | INFO     | __main__:main:62 - load_experiment         None
2024-08-13 09:41:12.951 | INFO     | __main__:main:62 - load_epoch              None
2024-08-13 09:41:12.951 | INFO     | __main__:main:62 - seed            3571997198
2024-08-13 09:41:12.952 | INFO     | __main__:main:62 - deterministic           False
2024-08-13 09:41:12.952 | INFO     | __main__:main:63 - ======================================================

2024-08-13 09:41:12.952 | INFO     | __main__:main:67 - [Train] Using device cuda
2024-08-13 09:41:12.952 | INFO     | __main__:main:76 - [Train] Do not enforce deterministic algorithms, cudnn benchmark enabled
2024-08-13 09:41:12.952 | INFO     | __main__:main:81 - [Train] Manual seed PyTorch with seed 3571997198

Parameter containing:
tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=torch.float64,
       requires_grad=True)
2024-08-13 09:41:13.273 | INFO     | evostencils.code_generation.trainer:__init__:138 - [Trainer] 10 training data loaded from /home/hadrien/Applications/mg_pytorch/evostencils/scripts/data/train
2024-08-13 09:41:13.273 | INFO     | evostencils.code_generation.trainer:__init__:148 - [Trainer] 0 evaluation data loaded from /home/hadrien/Applications/mg_pytorch/evostencils/scripts/data/evaluate

loss_x: tensor(6.8582, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0170, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0006, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0020, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0025, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0042, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0013, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0020, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0004, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0008, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
2024-08-13 09:41:39.416 | INFO     | evostencils.code_generation.trainer:train:209 - trainable omega = Parameter containing:
tensor([[0.6841, 0.6841, 0.6841, 0.6841, 0.6841, 0.6841, 0.6841, 0.6841, 0.6841,
         0.6841],
        [0.5279, 0.5279, 0.5279, 0.5279, 0.5279, 0.5279, 0.5279, 0.5279, 0.5279,
         0.5279],
        [1.1535, 1.1537, 1.1537, 1.1538, 1.1536, 1.1535, 1.1529, 1.1526, 1.1542,
         1.2382],
        [2.0635, 2.0635, 2.0635, 2.0635, 2.0635, 2.0635, 2.0635, 2.0635, 2.0635,
         2.0635],
        [1.9873, 1.9873, 1.9873, 1.9873, 1.9873, 1.9873, 1.9873, 1.9873, 1.9873,
         1.9873],
        [1.2272, 1.2272, 1.2272, 1.2272, 1.2272, 1.2272, 1.2272, 1.2272, 1.2272,
         1.2272],
        [1.4568, 1.4568, 1.4568, 1.4568, 1.4568, 1.4568, 1.4568, 1.4568, 1.4568,
         1.4568],
        [1.6783, 1.6783, 1.6783, 1.6783, 1.6783, 1.6783, 1.6783, 1.6783, 1.6783,
         1.6783],
        [0.8325, 0.8325, 0.8325, 0.8325, 0.8325, 0.8325, 0.8325, 0.8325, 0.8325,
         0.8325]], dtype=torch.float64, requires_grad=True), trainable weight = 0, time = 0.013831853866577148, conv_factor = 0.00048481927072528783, iterations_used = 1
loss_x: tensor(0.0001, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0016, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0096, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0036, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0070, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0011, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0002, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0018, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0008, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0013, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
2024-08-13 09:42:05.025 | INFO     | evostencils.code_generation.trainer:train:209 - trainable omega = Parameter containing:
tensor([[0.5834, 0.5834, 0.5834, 0.5834, 0.5834, 0.5834, 0.5834, 0.5834, 0.5834,
         0.5833],
        [0.9714, 0.9714, 0.9714, 0.9714, 0.9714, 0.9714, 0.9714, 0.9714, 0.9714,
         0.9714],
        [1.7602, 1.7426, 1.7423, 1.7311, 1.7476, 1.7610, 1.7199, 0.9715, 0.7266,
         0.8452],
        [2.2690, 2.2690, 2.2690, 2.2690, 2.2690, 2.2690, 2.2690, 2.2690, 2.2690,
         2.2690],
        [1.5757, 1.5757, 1.5757, 1.5757, 1.5757, 1.5757, 1.5757, 1.5757, 1.5757,
         1.5757],
        [1.0221, 1.0221, 1.0221, 1.0221, 1.0221, 1.0221, 1.0221, 1.0221, 1.0221,
         1.0221],
        [2.3411, 2.3411, 2.3411, 2.3411, 2.3411, 2.3411, 2.3411, 2.3411, 2.3411,
         2.3411],
        [1.6259, 1.6259, 1.6259, 1.6259, 1.6259, 1.6259, 1.6259, 1.6259, 1.6259,
         1.6259],
        [1.1245, 1.1245, 1.1245, 1.1245, 1.1245, 1.1245, 1.1245, 1.1245, 1.1245,
         1.1245]], dtype=torch.float64, requires_grad=True), trainable weight = 0, time = 0.014349699020385742, conv_factor = 0.002351405980346627, iterations_used = 1
loss_x: tensor(0.0165, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0006, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0016, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0005, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0003, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0004, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0003, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0002, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0016, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0009, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
2024-08-13 09:42:30.568 | INFO     | evostencils.code_generation.trainer:train:209 - trainable omega = Parameter containing:
tensor([[0.5092, 0.5092, 0.5092, 0.5092, 0.5092, 0.5092, 0.5092, 0.5092, 0.5092,
         0.5092],
        [1.1037, 1.1037, 1.1037, 1.1037, 1.1037, 1.1037, 1.1037, 1.1037, 1.1037,
         1.1037],
        [1.6438, 1.3575, 1.3636, 1.3719, 1.4043, 1.7274, 3.0147, 0.9957, 0.7528,
         0.7966],
        [2.3390, 2.3390, 2.3390, 2.3390, 2.3390, 2.3390, 2.3390, 2.3390, 2.3390,
         2.3390],
        [1.5215, 1.5215, 1.5215, 1.5215, 1.5215, 1.5215, 1.5215, 1.5215, 1.5215,
         1.5215],
        [1.0726, 1.0726, 1.0726, 1.0726, 1.0726, 1.0726, 1.0726, 1.0726, 1.0726,
         1.0726],
        [1.7367, 1.7367, 1.7367, 1.7367, 1.7367, 1.7367, 1.7367, 1.7367, 1.7367,
         1.7367],
        [2.0260, 2.0260, 2.0260, 2.0260, 2.0260, 2.0260, 2.0260, 2.0260, 2.0260,
         2.0260],
        [1.0752, 1.0752, 1.0752, 1.0752, 1.0752, 1.0752, 1.0752, 1.0752, 1.0752,
         1.0752]], dtype=torch.float64, requires_grad=True), trainable weight = 0, time = 0.013958454132080078, conv_factor = 0.0027631997596260297, iterations_used = 1
loss_x: tensor(0.0057, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0013, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0043, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0031, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0129, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0071, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0158, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0013, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0042, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0067, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
2024-08-13 09:42:55.728 | INFO     | evostencils.code_generation.trainer:train:209 - trainable omega = Parameter containing:
tensor([[0.6573, 0.6573, 0.6573, 0.6573, 0.6573, 0.6573, 0.6573, 0.6573, 0.6573,
         0.6573],
        [1.2527, 1.2527, 1.2527, 1.2527, 1.2527, 1.2527, 1.2527, 1.2527, 1.2527,
         1.2527],
        [0.7368, 3.6154, 3.6829, 1.6634, 2.7250, 0.9992, 0.9672, 0.5427, 0.6663,
         1.7025],
        [1.9216, 1.9216, 1.9216, 1.9216, 1.9216, 1.9216, 1.9216, 1.9216, 1.9216,
         1.9216],
        [1.9732, 1.9732, 1.9732, 1.9732, 1.9732, 1.9732, 1.9732, 1.9732, 1.9732,
         1.9732],
        [1.7292, 1.7292, 1.7292, 1.7292, 1.7292, 1.7292, 1.7292, 1.7292, 1.7292,
         1.7292],
        [1.2824, 1.2824, 1.2824, 1.2824, 1.2824, 1.2824, 1.2824, 1.2824, 1.2824,
         1.2824],
        [1.4073, 1.4073, 1.4073, 1.4073, 1.4073, 1.4073, 1.4073, 1.4073, 1.4073,
         1.4073],
        [0.7484, 0.7484, 0.7484, 0.7484, 0.7484, 0.7484, 0.7484, 0.7484, 0.7484,
         0.7484]], dtype=torch.float64, requires_grad=True), trainable weight = 0, time = 0.014188766479492188, conv_factor = 0.0015631856211239064, iterations_used = 1
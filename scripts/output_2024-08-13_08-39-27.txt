2024-08-13 09:38:28.062 | INFO     | __main__:main:60 - ======================== Args ========================
2024-08-13 09:38:28.062 | INFO     | __main__:main:62 - structure               unet
2024-08-13 09:38:28.062 | INFO     | __main__:main:62 - downsampling_policy             lerp
2024-08-13 09:38:28.062 | INFO     | __main__:main:62 - upsampling_policy               lerp
2024-08-13 09:38:28.062 | INFO     | __main__:main:62 - num_iterations          16
2024-08-13 09:38:28.063 | INFO     | __main__:main:62 - relative_tolerance              1e-06
2024-08-13 09:38:28.063 | INFO     | __main__:main:62 - initialize_x0           random
2024-08-13 09:38:28.063 | INFO     | __main__:main:62 - num_mg_layers           6
2024-08-13 09:38:28.063 | INFO     | __main__:main:62 - num_mg_pre_smoothing            2
2024-08-13 09:38:28.063 | INFO     | __main__:main:62 - num_mg_post_smoothing           2
2024-08-13 09:38:28.063 | INFO     | __main__:main:62 - activation              none
2024-08-13 09:38:28.063 | INFO     | __main__:main:62 - initialize_trainable_parameters         default
2024-08-13 09:38:28.063 | INFO     | __main__:main:62 - optimizer               LBFGS
2024-08-13 09:38:28.063 | INFO     | __main__:main:62 - scheduler               ['step', '1', '0.99']
2024-08-13 09:38:28.063 | INFO     | __main__:main:62 - initial_lr              0.05
2024-08-13 09:38:28.063 | INFO     | __main__:main:62 - lambda_1                1
2024-08-13 09:38:28.063 | INFO     | __main__:main:62 - lambda_2                1
2024-08-13 09:38:28.063 | INFO     | __main__:main:62 - start_epoch             0
2024-08-13 09:38:28.063 | INFO     | __main__:main:62 - max_epoch               1
2024-08-13 09:38:28.063 | INFO     | __main__:main:62 - save_every              1
2024-08-13 09:38:28.064 | INFO     | __main__:main:62 - evaluate_every          5
2024-08-13 09:38:28.064 | INFO     | __main__:main:62 - dataset_root            /home/hadrien/Applications/mg_pytorch/evostencils/scripts/data/
2024-08-13 09:38:28.064 | INFO     | __main__:main:62 - num_workers             22
2024-08-13 09:38:28.064 | INFO     | __main__:main:62 - batch_size              1
2024-08-13 09:38:28.064 | INFO     | __main__:main:62 - device          cuda
2024-08-13 09:38:28.064 | INFO     | __main__:main:62 - checkpoint_root         /home/hadrien/Applications/mg_pytorch/evostencils/scripts/train/checkpoints
2024-08-13 09:38:28.064 | INFO     | __main__:main:62 - load_experiment         None
2024-08-13 09:38:28.064 | INFO     | __main__:main:62 - load_epoch              None
2024-08-13 09:38:28.064 | INFO     | __main__:main:62 - seed            2620021250
2024-08-13 09:38:28.064 | INFO     | __main__:main:62 - deterministic           False
2024-08-13 09:38:28.064 | INFO     | __main__:main:63 - ======================================================

2024-08-13 09:38:28.064 | INFO     | __main__:main:67 - [Train] Using device cuda
2024-08-13 09:38:28.064 | INFO     | __main__:main:76 - [Train] Do not enforce deterministic algorithms, cudnn benchmark enabled
2024-08-13 09:38:28.065 | INFO     | __main__:main:81 - [Train] Manual seed PyTorch with seed 2620021250

Parameter containing:
tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=torch.float64,
       requires_grad=True)
2024-08-13 09:38:28.392 | INFO     | evostencils.code_generation.trainer:__init__:138 - [Trainer] 10 training data loaded from /home/hadrien/Applications/mg_pytorch/evostencils/scripts/data/train
2024-08-13 09:38:28.392 | INFO     | evostencils.code_generation.trainer:__init__:148 - [Trainer] 0 evaluation data loaded from /home/hadrien/Applications/mg_pytorch/evostencils/scripts/data/evaluate

loss_x: tensor(3.6751, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0163, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0015, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0090, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0008, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0019, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0040, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0004, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0019, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0021, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
2024-08-13 09:38:54.677 | INFO     | evostencils.code_generation.trainer:train:209 - trainable omega = Parameter containing:
tensor([[0.4335, 0.4335, 0.4335, 0.4335, 0.4335, 0.4335, 0.4335, 0.4335, 0.4335,
         0.4335],
        [0.4637, 0.4637, 0.4637, 0.4637, 0.4637, 0.4637, 0.4637, 0.4637, 0.4637,
         0.4637],
        [1.1020, 1.1020, 1.1020, 1.1020, 1.1019, 1.1016, 1.1026, 1.1040, 1.1054,
         1.1098],
        [2.0633, 2.0633, 2.0633, 2.0633, 2.0633, 2.0633, 2.0633, 2.0633, 2.0633,
         2.0633],
        [1.9719, 1.9719, 1.9719, 1.9719, 1.9719, 1.9719, 1.9719, 1.9719, 1.9719,
         1.9719],
        [1.7684, 1.7684, 1.7684, 1.7684, 1.7684, 1.7684, 1.7684, 1.7684, 1.7684,
         1.7684],
        [1.5307, 1.5307, 1.5307, 1.5307, 1.5307, 1.5307, 1.5307, 1.5307, 1.5307,
         1.5307],
        [1.3287, 1.3287, 1.3287, 1.3287, 1.3287, 1.3287, 1.3287, 1.3287, 1.3287,
         1.3287],
        [1.0966, 1.0966, 1.0966, 1.0966, 1.0966, 1.0966, 1.0966, 1.0966, 1.0966,
         1.0966]], dtype=torch.float64, requires_grad=True), trainable weight = 0, time = 0.013769149780273438, conv_factor = 0.001059638766765189, iterations_used = 1
loss_x: tensor(0.0015, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0018, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0026, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0035, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0005, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0003, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0021, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0075, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0028, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0008, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
2024-08-13 09:39:20.555 | INFO     | evostencils.code_generation.trainer:train:209 - trainable omega = Parameter containing:
tensor([[0.6043, 0.6043, 0.6043, 0.6043, 0.6043, 0.6043, 0.6043, 0.6043, 0.6043,
         0.6043],
        [0.6366, 0.6366, 0.6366, 0.6366, 0.6366, 0.6366, 0.6366, 0.6366, 0.6366,
         0.6366],
        [0.9941, 0.9921, 0.9940, 0.9993, 1.0581, 0.7274, 1.0746, 1.3555, 2.1044,
         3.5652],
        [2.2804, 2.2804, 2.2804, 2.2804, 2.2804, 2.2804, 2.2804, 2.2804, 2.2804,
         2.2804],
        [1.9300, 1.9300, 1.9300, 1.9300, 1.9300, 1.9300, 1.9300, 1.9300, 1.9300,
         1.9300],
        [1.2761, 1.2761, 1.2761, 1.2761, 1.2761, 1.2761, 1.2761, 1.2761, 1.2761,
         1.2761],
        [2.0270, 2.0270, 2.0270, 2.0270, 2.0270, 2.0270, 2.0270, 2.0270, 2.0270,
         2.0270],
        [1.1220, 1.1220, 1.1220, 1.1220, 1.1220, 1.1220, 1.1220, 1.1220, 1.1220,
         1.1220],
        [1.1034, 1.1034, 1.1034, 1.1034, 1.1034, 1.1034, 1.1034, 1.1034, 1.1034,
         1.1034]], dtype=torch.float64, requires_grad=True), trainable weight = 0, time = 0.014315128326416016, conv_factor = 0.0006812919544056285, iterations_used = 1
loss_x: tensor(0.0023, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0046, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0005, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0003, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0008, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0014, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0004, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0017, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0005, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0010, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
2024-08-13 09:39:46.194 | INFO     | evostencils.code_generation.trainer:train:209 - trainable omega = Parameter containing:
tensor([[0.7460, 0.7460, 0.7460, 0.7460, 0.7460, 0.7460, 0.7460, 0.7460, 0.7460,
         0.7459],
        [1.0662, 1.0662, 1.0662, 1.0662, 1.0662, 1.0662, 1.0662, 1.0662, 1.0662,
         1.0662],
        [1.3070, 0.9173, 0.9567, 0.4954, 1.1006, 0.6032, 0.9522, 1.5141, 2.0223,
         4.0210],
        [2.5660, 2.5660, 2.5660, 2.5660, 2.5660, 2.5660, 2.5660, 2.5660, 2.5660,
         2.5660],
        [1.9185, 1.9185, 1.9185, 1.9185, 1.9185, 1.9185, 1.9185, 1.9185, 1.9185,
         1.9185],
        [0.8550, 0.8550, 0.8550, 0.8550, 0.8550, 0.8550, 0.8550, 0.8550, 0.8550,
         0.8550],
        [1.9712, 1.9712, 1.9712, 1.9712, 1.9712, 1.9712, 1.9712, 1.9712, 1.9712,
         1.9712],
        [1.6055, 1.6055, 1.6055, 1.6055, 1.6055, 1.6055, 1.6055, 1.6055, 1.6055,
         1.6055],
        [0.9898, 0.9898, 0.9898, 0.9898, 0.9898, 0.9898, 0.9898, 0.9898, 0.9898,
         0.9898]], dtype=torch.float64, requires_grad=True), trainable weight = 0, time = 0.01583576202392578, conv_factor = 0.0021697467734549457, iterations_used = 1
loss_x: tensor(0.0017, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0015, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0002, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0018, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0054, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0002, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0019, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0003, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0009, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
loss_x: tensor(0.0004, device='cuda:0', dtype=torch.float64,
       grad_fn=<MseLossBackward0>)
2024-08-13 09:40:11.446 | INFO     | evostencils.code_generation.trainer:train:209 - trainable omega = Parameter containing:
tensor([[0.7503, 0.7503, 0.7503, 0.7503, 0.7503, 0.7503, 0.7503, 0.7502, 0.7503,
         0.7502],
        [1.0560, 1.0560, 1.0560, 1.0560, 1.0560, 1.0560, 1.0560, 1.0560, 1.0560,
         1.0560],
        [1.3914, 0.8558, 1.0625, 0.7254, 1.2374, 0.8226, 0.8650, 1.5227, 2.2073,
         8.1032],
        [2.6979, 2.6979, 2.6979, 2.6979, 2.6979, 2.6979, 2.6979, 2.6979, 2.6979,
         2.6979],
        [0.5536, 0.5536, 0.5536, 0.5536, 0.5536, 0.5536, 0.5536, 0.5536, 0.5536,
         0.5536],
        [1.4766, 1.4766, 1.4766, 1.4766, 1.4766, 1.4766, 1.4766, 1.4766, 1.4766,
         1.4766],
        [1.0791, 1.0791, 1.0791, 1.0791, 1.0791, 1.0791, 1.0791, 1.0791, 1.0791,
         1.0791],
        [1.8557, 1.8557, 1.8557, 1.8557, 1.8557, 1.8557, 1.8557, 1.8557, 1.8557,
         1.8557],
        [0.7674, 0.7674, 0.7674, 0.7674, 0.7674, 0.7674, 0.7674, 0.7674, 0.7674,
         0.7674]], dtype=torch.float64, requires_grad=True), trainable weight = 0, time = 0.014336347579956055, conv_factor = 0.0006489617428527864, iterations_used = 1